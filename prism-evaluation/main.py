import argparse
import json
import sys
import warnings
from dataclasses import dataclass
from typing import Any, Dict, List, Set, Tuple, Union

import numpy as np
import pandas as pd
from polygon import ReferenceClient, StocksClient

warnings.filterwarnings("ignore")

# Scaling constants for calculating points
# see docs/scoring.md for more info
ROI_SCALE = 12
DIVERSITY_SCALE = 12
CLI_SAT_SCALE = 20
RAR_SCALE = 12

# Target Volatility configuration
CLIENT_SAT_TARGET_VOL_DEFAULT = 0.08

# Age tolerance configuration
AGE_YOUNG = 30
AGE_MID = 50
AGE_YOUNG_DIVISOR = 12  # (age - AGE_MIN) / AGE_YOUNG_DIVISOR for ramp up
AGE_OLD_DIVISOR = 20    # (AGE_MAX - age) / AGE_OLD_DIVISOR for decline

# Employment status configuration
UNEMPLOYED_RISK_FACTOR = 1.2

# DONT CHANGE
DEBUG = False
AGE_MIN = 18
AGE_MAX = 80
AGE_TOL_DEFAULT = 0.6


@dataclass
class Context:
    """Context struct"""

    timestamp: str
    start: str
    end: str
    age: int
    employed: bool
    salary: float
    budget: float
    dislikes: set


def mse_from_ideal(weights_map: list[int]):
    if len(weights_map) == 1:
        # One stock is NOT a portfolio, punish
        weights_map.append(10000000)

    weights = np.array(weights_map, dtype=float)
    n = weights.size

    if n == 0:
        return 0.0

    ideal = weights.sum() / n
    mse = np.mean((weights - ideal) ** 2)

    return mse


def get_tickers_agg_bars(
    client: StocksClient, data: List[Tuple[str, int]], start: str, end: str
) -> pd.DataFrame | None:
    df = pd.DataFrame(None, columns=["ticker", "v", "vw", "o", "c", "h", "l", "t", "n"])
    problematic_tickers = []
    for ticker, qty in data:
        bars = client.get_aggregate_bars(
            ticker, from_date=start, to_date=end, timespan="day"
        )
        if "results" not in bars:
            problematic_tickers.append(ticker)
            continue
        tmpdf = pd.DataFrame(bars["results"])
        tmpdf["ticker"] = ticker
        tmpdf["qty"] = qty

        df = pd.concat([df, tmpdf])
    df.set_index(["t", "ticker"], inplace=True)
    df["value"] = df["c"] * df["qty"]
    return df, problematic_tickers


def init_price_breaches_threshold(df: pd.DataFrame, threshold: float) -> (bool, float):
    value = df.groupby(level=1).first()["value"].sum()
    return (value > threshold, value)


def is_industry_in_dislikes(
    stock: str,
    ticker_details: Dict[str, Dict],
    context: Context,
    sic_industry: Dict[str, List[str]],
) -> bool:
    stock_details = ticker_details[stock]
    sic_code = stock_details["results"]["sic_code"]
    for industry in sic_industry[sic_code]:
        if industry in context.dislikes:
            return True
    return False


def sharpe(
    df: pd.DataFrame,
    rates: pd.DataFrame,
    days: int = 252,
):

    # Group the values; this returns a Series
    values = df.groupby(level=0)["value"].apply(lambda x: x.sum())
    values.index = pd.to_datetime(values.index, unit="ms")

    # Convert the Series to a DataFrame
    values = values.to_frame(name="value")

    # Ensure both indexes are sorted
    values = values.sort_index()
    rates = rates.sort_index()

    nearest_idx = rates.index.get_indexer(values.index, method="nearest")

    values["returns"] = (values["value"] / values["value"].shift(1)) - 1
    daily_risk_free = ((1 + rates.iloc[nearest_idx]["value"] / 100) ** (1 / 252)) - 1
    values["risk_free_rate"] = daily_risk_free.values
    values["diff"] = values["returns"] - values["risk_free_rate"]

    value = values["diff"].mean() / values["diff"].std()
    value *= np.sqrt(days)
    return value


def sortino(
    df: pd.DataFrame,
    context: Context,
    rates: pd.DataFrame,
    days: int = 252,
):
    """
    Calculate the Sortino ratio which only considers downside volatility.
    """
    values = df.groupby(level=0)["value"].apply(lambda x: x.sum())
    values.index = pd.to_datetime(values.index, unit="ms")
    values = values.to_frame(name="value")
    values = values.sort_index()
    rates = rates.sort_index()
    nearest_idx = rates.index.get_indexer(values.index, method="nearest")

    values["returns"] = values["value"].pct_change()
    daily_risk_free = ((1 + rates.iloc[nearest_idx]["value"] / 100) ** (1 / 252)) - 1
    values["risk_free_rate"] = daily_risk_free.values
    values["excess_returns"] = values["returns"] - values["risk_free_rate"]

    # Only consider negative excess returns for downside risk
    downside_returns = values["excess_returns"].where(values["excess_returns"] < 0, 0)
    downside_deviation = downside_returns.std()

    mean_excess_return = values["excess_returns"].mean()
    # Avoid division by zero; if no downside volatility, default to Sharpe
    if downside_deviation == 0:
        return np.nan
    sortino_ratio = mean_excess_return / downside_deviation * np.sqrt(days)
    return sortino_ratio


def risk_profile(context: Context) -> float:
    """Risk profile returns a float from [0, 1.2] where a higher score indicates
    more a more risk_averse client"""

    def age_profile(age):
        x0 = 45  # inflection age
        k = 0.1  # steepness factor
        risk_tolerance_logistic = 1 / (1 + np.exp(k * (age - x0)))
        return risk_tolerance_logistic

    salary_budget_ratio = min(context.budget, context.salary) / max(1, context.salary)

    risk_factor = (age_profile(context.age) + salary_budget_ratio) / 2

    # FIXME: Think about employment_status harder.
    if not context.employed:
        risk_factor *= 1.2

    return risk_factor


def get_points(
    df: pd.DataFrame,
    profit: float,
    stocks: List[Tuple[str, int]],
    legal_stocks: list[tuple[str, int]],
    context: Context,
    unique_industries: Set[str],
    basedir: str,
    sic_industry: dict[str, list[str]],
    ticker_details: dict,
) -> int:
    n_allowed_industries = len(unique_industries) - len(context.dislikes)

    roi = return_on_investment(profit, context)
    diversity = diversity_score(
        df,
        stocks,
        unique_industries,
        n_allowed_industries,
        sic_industry,
        ticker_details,
    )
    client_sat = client_satisfaction(df, risk_profile(context))
    rar = risk_adjusted_returns(df, context, basedir)

    if DEBUG:
        print(f"{roi=}", f"{diversity=}", f"{client_sat=}", f"{rar=}")

    points = (
        ROI_SCALE * roi
        + DIVERSITY_SCALE * diversity
        + CLI_SAT_SCALE * client_sat
        + RAR_SCALE * rar
    )

    value = df.groupby(level=1).first()["value"].sum()
    points *= 1 - (value / context.budget)

    points = points * 10  # make a nicer scale for points

    # Dock fixed amount of points, if illegal
    if len(legal_stocks) != len(stocks):
        diff = abs(len(legal_stocks) - len(stocks))
        profit = -(0.1 * diff) * profit if profit > 0 else profit * (1 + (0.1 * diff))
        points = -(0.1 * diff) * points if points > 0 else points * (1 + (0.1 * diff))

    return points if profit > 0 else -abs(points)


def return_on_investment(profit: float, context: Context) -> float:
    """Return on investment is defined as the ratio of profit to initial budget"""
    # return np.log(profit) if profit > 0 else -np.log(abs(profit)) if profit < 0 else 0
    return profit / context.budget


def diversity_score(
    df: pd.DataFrame,
    stocks: List[Tuple[str, int]],
    unique_industries: set[str],
    n_allowed_industries: int,
    sic_industry: dict[str, list[str]],
    ticker_details: dict,
) -> float:

    stock_quantity_prods = df.groupby(level=1).first()["value"]
    unique_stocks = set([s for s, _ in stocks])
    stock_counts = [stock_quantity_prods[s] for s in unique_stocks]
    stocks_per_industry = stock_count_per_industry(ticker_details, stocks, sic_industry)
    stock_mse = mse_from_ideal(stock_counts)
    sic_mse = mse_from_ideal(list(stocks_per_industry.values()))

    stock_score = np.log(1 + len(stocks)) / (1 + len(unique_stocks) * stock_mse)
    sic_score = (len(unique_industries) / n_allowed_industries) / (1 + sic_mse)

    return stock_score + sic_score


def stock_count_per_industry(
    ticker_details: dict,
    stocks: list[tuple[str, int]],
    sic_industry: dict[str, list[str]],
) -> dict[str, int]:
    """Returns how many stocks were invested per industry"""
    industry_count_map: dict[str, int] = {}

    # XXX: sic_industries map a code to a list of industries in a (theoretically)
    # many-many fashion. Visual inspection suggests that
    # len(sic_industries[TCKR]) is always one. This makes calculating
    # diversification harder. To workaround this, always assign stocks to the
    # **first industry** in the list.

    for tkr, quantity in stocks:
        sic_code = ticker_details[tkr]["results"]["sic_code"]
        industries = sic_industry[sic_code]

        if not industries:
            industry = "unmapped"
        else:
            industry = industries[0]

        if industry not in industry_count_map:
            industry_count_map[industry] = 0

        industry_count_map[industry] += quantity

    return industry_count_map


def client_satisfaction(df: pd.DataFrame, risk_profile: float, age: int | None = None, target_vol: float = CLIENT_SAT_TARGET_VOL_DEFAULT) -> float:
    """Client satisfaction score in [0,1] using returns-based volatility."""
    r = df["value"].pct_change().dropna()
    if r.size == 0:
        return 0.0
    portfolio_vol = r.std()
    if not np.isfinite(portfolio_vol):
        return 0.0

    rp = max(0.0, min(1.2, risk_profile))
    base_from_risk = 0.9 - 0.6 * (rp / 1.2)

    def age_tolerance(a: int | None) -> float:
        if a is None:
            return AGE_TOL_DEFAULT
        if a < AGE_MIN or a >= AGE_MAX:
            return 0.0
        if a <= AGE_YOUNG:
            return (a - AGE_MIN) / AGE_YOUNG_DIVISOR
        if a <= AGE_MID:
            return 1.0
        return (AGE_MAX - a) / AGE_OLD_DIVISOR

    combined_factor = max(0.05, base_from_risk * age_tolerance(age))
    tolerance = target_vol * combined_factor

    if portfolio_vol <= tolerance:
        return 1.0

    score = 1 - (portfolio_vol - tolerance) / max(tolerance, 1e-9)
    return float(max(0.0, min(1.0, score)))


def risk_adjusted_returns(
    df: pd.DataFrame,
    context: Context,
    basedir: str,
) -> float:
    """Calculate the risk-adjusted returns using Sharpe and Sortino ratios"""
    rates = pd.read_csv(f"{basedir}/bond-rate.csv")
    rates.set_index("date", inplace=True)
    rates.index = pd.to_datetime(rates.index)

    rar = sharpe(df, rates)
    s_ratio = sortino(df, context, rates)
    if np.isnan(s_ratio):
        risk_adjusted = rar
    else:
        risk_adjusted = (rar + s_ratio) / 2

    return risk_adjusted


def evaluate(
    df: pd.DataFrame,
    stocks: List[Tuple[str, int]],
    context: Context,
    sic_industry: Dict[str, List[str]],
    unique_industries: Set[str],
    ref_client: ReferenceClient,
    basedir: str,
) -> Tuple[bool, str, float, int]:
    # Check did not send multiple stocks
    if len(stocks) != len(set([s for s, _ in stocks])):
        return False, f"Error: duplicate tickers: {stocks}", 0.0, -1

    if min([i for _, i in stocks]) <= 0:
        return False, f"Error: invalid stock weight: {stocks}", 0.0, -1

    failure, value = init_price_breaches_threshold(df, context.budget)
    if failure:
        # Breached the max price that someone has
        return (
            False,
            f"Error: budget breached (your portfolio value: {value}, budget: {context.budget})",
            0.0,
            -1,
        )

    # Verify all tickers exist and grab details
    ticker_details = {}
    for stock, _ in stocks:
        details = ref_client.get_ticker_details(stock)
        if details["status"] != "OK":
            return False, f"Error: invalid ticker: {stock}", 0.0, -1
        elif details["results"]["type"] != "CS":
            return (
                False,
                f"Error: invalid ticker type: {stock} of type {details['results']['type']}",
                0.0,
                -1,
            )
        ticker_details[stock] = details

    # Remove stocks if they are not legal, so we only calculate using the legal stocks.
    legal_stocks = [
        (stock, qty)
        for stock, qty in stocks
        if not is_industry_in_dislikes(stock, ticker_details, context, sic_industry)
    ]

    profit = (
        df.groupby(level=1).last()["value"].sum()
        - df.groupby(level=1).first()["value"].sum()
    )

    points = get_points(
        df,
        profit,
        stocks,
        legal_stocks,
        context,
        unique_industries,
        basedir,
        sic_industry,
        ticker_details,
    )

    return True, "", profit, points


def main(api_key: str, data: Dict[str, Union[List[Dict[str, int]], Any]]):
    stocks_client = StocksClient(api_key)
    ref_client = ReferenceClient(api_key)
    if "context" not in data:
        print("context not passed through")
        return

    with open(f"{args.basedir}/sic_industry.json", "r") as f:
        sic_industry = json.loads(f.read())
    with open(f"{args.basedir}/unique_industries.json", "r") as f:
        unique_industries = json.loads(f.read())

    context = Context(**data["context"])

    stocks = []
    for stock in data["stocks"]:
        stocks.append([stock["ticker"], stock["quantity"]])

    df, problematic_tickers = get_tickers_agg_bars(
        stocks_client,
        stocks,
        # done-todo: replace string slicing with proper parsing
        #   https://stackoverflow.com/questions/1941927/convert-an-rfc-3339-time-to-a-standard-python-timestamp
        start=context.start,
        end=context.end,
    )
    if df is None or len(problematic_tickers):
        print(
            json.dumps(
                {
                    "passed": False,
                    "profit": 0.0,
                    "points": -1.0,
                    "error": f"invalid ticker(s) passed in {stocks}. The error here either means you have passed in invalid ticker(s) OR the tickers are not valid for the time range provided. Please ensure that the ticker(s) is (are) trading publicly during the ENTIRE time frame provided. Additionally, make sure you are using the CANNONICAL TICKER. Problematic Stocks: {problematic_tickers}",
                }
            )
        )
        return

    passed, error_message, profit, points = evaluate(
        df, stocks, context, sic_industry, unique_industries, ref_client, args.basedir
    )

    print(
        json.dumps(
            {
                "passed": passed,
                "profit": profit,
                "points": 0 if np.isnan(points) else points,
                "error": error_message,
            }
        )
    )


if __name__ == "__main__":
    try:
        parser = argparse.ArgumentParser()
        parser.add_argument("--apikey", help="The polygon api key", required=True)
        parser.add_argument(
            "--basedir", help="Directory base to look for files.", default="./"
        )
        args = parser.parse_args()

        data = json.loads(sys.stdin.read())
        main(args.apikey, data)
    except Exception as e:
        print(e)
        sys.exit(1)

        